{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing basic required libraries\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing the data from google drive\n",
    "\n",
    "train_url = 'https://drive.google.com/file/d/1UiFmiaLmD4CKbXh9xWqiUCi-5Vq6ce3_/view?usp=sharing'\n",
    "\n",
    "train_download_url = 'https://drive.google.com/uc?id=' + train_url.split('/')[-2]\n",
    "\n",
    "test_url = 'https://drive.google.com/file/d/1Q-YuLzD9M4d7cIcD48LzOJubSY9Poi5A/view?usp=sharing'\n",
    "\n",
    "test_download_url = 'https://drive.google.com/uc?id=' + test_url.split('/')[-2]\n",
    "\n",
    "\n",
    "train = pd.read_csv(train_download_url)\n",
    "test = pd.read_csv(test_download_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking out the train and test data\n",
    "\n",
    "print('Train data:\\n')\n",
    "print(train.head())\n",
    "print()\n",
    "print('Test data\\n')\n",
    "print(test.head())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking out the number of rows and columns in the data\n",
    "\n",
    "print(f\"The number of rows in the train data is {train.shape[0]}.\\n\")\n",
    "print(f\"The number of columns in the train data is {train.shape[1]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The number of rows in the test data is {test.shape[0]}.\\n\")\n",
    "print(f\"The number of columns in the test data is {test.shape[1]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## understanding the data types of features \n",
    "\n",
    "train.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Looks like all the independent features are of int64 datatype.</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's find out the number of unique values in each of the independent features\n",
    "\n",
    "for feature in train.columns:\n",
    "    print(f\"The unique values in the feature {feature} are {train[feature].unique()} (total of {train[feature].nunique()}).\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finding out the features with less than 50 unique values (These will be categorical features which are encoded as )\n",
    "\n",
    "print('Features with less than 50 unique values are: \\n')\n",
    "for feature in train.columns:\n",
    "    if train[feature].nunique() <= 50:\n",
    "        print(f\"{feature}\", end=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## In the dataset, null values are represented as 'na'. Let's convert them to np.nan\n",
    "\n",
    "def miss(x):\n",
    "    if x == 'na':\n",
    "        return np.nan\n",
    "\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "\n",
    "for feature in train.columns[1:]:\n",
    "    train[feature] = train[feature].map(miss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking for the missing values in the data\n",
    "\n",
    "missing_values_df = pd.DataFrame()\n",
    "missing_values_df['Features'] = train.columns\n",
    "missing_values_df['Number_of_missing_values'] = train.isnull().sum().to_numpy()\n",
    "missing_values_df['Percentage_of_missing_values (%)'] = missing_values_df['Number_of_missing_values'].apply(lambda x: np.round((x/train.shape[0])*100),2)\n",
    "missing_values_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## finding the features having more than 20% missing values\n",
    "\n",
    "useless_feat = dict()\n",
    "for feature in train.columns[1:]:\n",
    "    if train[feature].isnull().sum()/train.shape[0] >= 0.2:\n",
    "        useless_feat[feature] = train[feature].isnull().sum()/train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useless_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(columns=useless_feat.keys(), axis=1, inplace=True)\n",
    "test.drop(columns=useless_feat.keys(), axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Since there are too many features, instead of finding if each of the feature contains outlier we will try to train the model with and without removing outliers and then check the results.</strong>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Also, to avoid overfitting, let's use medium criteria while replacing missing values in the remaining data.</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking if the dataset is imbalanced or not\n",
    "\n",
    "train['class'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>It seems the data is highly imbalanced. So, let's use the machine learning algorithms that are immune to this.</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scania_truck",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1e5dcf25b52cc7f1d3aa21efa94c6afdd1cec9dc3b4c8637850b5d853d244da2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
